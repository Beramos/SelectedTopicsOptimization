{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained convex optimization\n",
    "\n",
    "*Selected Topics in Mathematical Optimization: 2017-2018*\n",
    "\n",
    "**Michiel Stock** ([email](michiel.stock@ugent.be))\n",
    "\n",
    "![](Figures/logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from teachingtools import plot_contour, add_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equality constrained convex optimization\n",
    "\n",
    "### Problem outline\n",
    "\n",
    "We will start with convex optimization problems with linear equality constraints:\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x} f(\\mathbf{x}) \\\\\n",
    "\\text{subject to } A\\mathbf{x}=\\mathbf{b}\n",
    "$$\n",
    "\n",
    "where $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is convex and twice continuously differentiable and $A\\in \\mathbb{R}^{p\\times n}$ with a rank $p < n$.\n",
    "\n",
    "The Lagrangian of this problem is\n",
    "\n",
    "$$\n",
    "L(\\mathbf{x}, \\boldsymbol{\\nu}) = f(\\mathbf{x}) + \\boldsymbol{\\nu}^\\top(A\\mathbf{x}-\\mathbf{b})\\,,\n",
    "$$\n",
    "with $\\boldsymbol{\\nu}\\in\\mathbb{R}^p$ the vector of Lagrange multipliers.\n",
    "\n",
    "A point $\\mathbf{x}^\\star\\in$ **dom** $f$ is optimal for the above optimization problem only if there is a $\\boldsymbol{\\nu}^\\star\\in\\mathbb{R}^p$ such that:\n",
    "\n",
    "$$\n",
    "A\\mathbf{x}^\\star = \\mathbf{b}, \\qquad \\nabla f(\\mathbf{x}^\\star) + A^\\top\\boldsymbol{\\nu}^\\star = 0\\,.\n",
    "$$\n",
    "\n",
    "We will reuse the same toy examples from the previous chapter, but add an equality constraint to both.\n",
    "\n",
    "- Simple quadratic problem:\n",
    "\n",
    "$$\n",
    " \\min_{\\mathbf{x}} \\frac{1}{2} (x_1^2 + 4 x_2^2)\\\\\n",
    " \\text{subject to }  x_1 - 2x_2 = 3\n",
    "$$\n",
    "\n",
    "- A non-quadratic function:\n",
    "\n",
    "$$\n",
    " \\min_{\\mathbf{x}}\\log(e^{x_1 +3x_2-0.1}+e^{x_1 -3x_2-0.1}+e^{-x_1 -0.1})\\\\\n",
    " \\text{subject to }  x_1 + 3x_2 = 0  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teachingtools import quadratic, grad_quadratic, hessian_quadratic\n",
    "from teachingtools import nonquadratic, grad_nonquadratic, hessian_nonquadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "plot_contour(quadratic, (-11, 11), (-5, 5), ax1, [1.0, -2.0], 3)\n",
    "ax1.set_title('Quadratic')\n",
    "ax1.legend(loc=0)\n",
    "plot_contour(nonquadratic, (-2, 2), (-1, 1), ax2, [1, 3], 0)\n",
    "ax2.set_title('Non-quadratic')\n",
    "ax2.legend(loc=0)\n",
    "ax1.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality constrained convex quadratic optimization\n",
    "\n",
    "Consider the following equality constrained convex optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x}\\frac{1}{2}\\mathbf{x}^\\top P \\mathbf{x} + \\mathbf{q}^\\top \\mathbf{x} + r  \\\\\n",
    "\\text{subject to }  A\\mathbf{x}=\\mathbf{b}\n",
    "$$\n",
    "\n",
    "where $P$ is symmetric.\n",
    "\n",
    "The optimality conditions are\n",
    "$$\n",
    "A\\mathbf{x}^\\star = \\mathbf{b}, \\quad P\\mathbf{x}^\\star+\\mathbf{q} +A^\\top\\boldsymbol{\\nu}^\\star=\\mathbf{0}\\,,\n",
    "$$\n",
    "which we can write as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "P & A^\\top \\\\\n",
    "A & 0 \\\\\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "\\mathbf{x}^\\star\\\\\n",
    "\\boldsymbol{\\nu}^\\star\n",
    "     \\end{bmatrix}\n",
    "     =\n",
    "     \\begin{bmatrix}\n",
    "-\\mathbf{q} \\\\\n",
    "\\mathbf{b}\n",
    "     \\end{bmatrix}\\,.\n",
    "$$\n",
    "Note that this is a block matrix.\n",
    "\n",
    "> If $P$ is positive-definite, the linearly constrained quadratic minimization problem has an unique solution.\n",
    "\n",
    "Solving this linear system gives both the constrained minimizer $\\mathbf{x}^\\star$ as well as the Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1**\n",
    "\n",
    "1. Complete the code to solve linearly constrained quadratic systems.\n",
    "2. Use this code to solve the quadratic toy problem defined above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from constrained import solve_constrained_quadratic_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_constrained_quadratic_problem(P, q, A, b):\n",
    "    \"\"\"\n",
    "    Solve a linear constrained quadratic convex problem.\n",
    "\n",
    "    Inputs:\n",
    "        - P, q: quadratic and linear parameters of\n",
    "                the linear function to be minimized\n",
    "        - A, b: system of the linear constraints\n",
    "\n",
    "    Outputs:\n",
    "        - xstar: the exact minimizer\n",
    "        - vstar: the optimal Lagrange multipliers\n",
    "    \"\"\"\n",
    "    p, n = ...  # size of the problem\n",
    "    # complete this code\n",
    "    # HINT: use np.linalg.solve and np.bmat\n",
    "    solution = ...\n",
    "    xstar = solution[:n]\n",
    "    vstar = solution[n:]\n",
    "    return np.array(xstar), np.array(vstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the parameters\n",
    "\n",
    "P = ...  #  2 x 2 array\n",
    "A = ...  #  1 x 2 array\n",
    "b = ...  # 1 x 1 array\n",
    "q = ...  #  2 x 1 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar, vstar = solve_constrained_quadratic_problem(...  # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimizer:')\n",
    "print(xstar)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contour(quadratic, (-11, 11), (-5, 5), ax, [1, -2], 3)\n",
    "ax.scatter(xstar[0,0], xstar[1,0], 50, 'r', label='minimum')\n",
    "ax.grid()\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method with equality constraints\n",
    "\n",
    "To derive $\\Delta \\mathbf{x}_{nt}$ for the following equality constrained problem\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x}  f(\\mathbf{x}) \\\\\n",
    "\\text{subject to }  A\\mathbf{x}=\\mathbf{b}\n",
    "$$\n",
    "\n",
    "we apply a second-order Taylor approximation at the point $\\mathbf{x}$, to obtain\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{x} \\hat{f}(\\mathbf{x} +\\mathbf{v}) = f(\\mathbf{x}) +\\nabla f(\\mathbf{x})^\\top \\mathbf{v}+ \\frac{1}{2}\\mathbf{v}^\\top \\nabla^2 f(\\mathbf{x}) \\mathbf{v} \\\\\n",
    "\\text{subject to } A(\\mathbf{x}+\\mathbf{v})=\\mathbf{b}\\,.\n",
    "$$\n",
    "\n",
    "Based on the solution of quadratic convex problems with linear constraints, the Newton $\\Delta \\mathbf{x}_{nt}$ step is characterized by\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " \\nabla^2 f(\\mathbf{x})&  A^\\top \\\\\n",
    "A & 0 \\\\\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "\\Delta \\mathbf{x}_{nt}\\\\\n",
    "\\mathbf{w}\n",
    "     \\end{bmatrix}\n",
    "     =\n",
    "     -\\begin{bmatrix}\n",
    "\\nabla f(\\mathbf{x}) \\\\\n",
    "A\\mathbf{x}-\\mathbf{b}\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- If the starting point $\\mathbf{x}^{(0)}$ is chosen such that $A\\mathbf{x}^{(0)}=\\mathbf{b}$, the residual term vanishes and steps will remain in the feasible region. This is the **feasible start Newton method**.\n",
    "- If we choose an arbitrary $\\mathbf{x}^{(0)}\\in$ **dom** $f$, not satisfying the constraints, this is the **infeasible start Newton method**. It will usually converge rapidly to the feasible region (check the final solution!).\n",
    "\n",
    "Note that when we start at a feasible point, the residual vector $-(A\\mathbf{x}-\\mathbf{b})$ vanishes and the path will always remain in a feasible region. Otherwise we will converge to it.\n",
    "\n",
    "In this chapter, we will use a fixed step size. For Newton's method this usually leads to only a few extra iterations compared to an adaptive step size.\n",
    "\n",
    ">**input** starting point $\\mathbf{x}\\in$ **dom** $f$ (with $A\\mathbf{x}=\\mathbf{b}$ if using the feasible method), tolerance $\\epsilon>0$.\n",
    ">\n",
    ">**repeat**\n",
    ">\n",
    ">>    1. Compute the Newton step $\\Delta \\mathbf{x}_{nt}$ and decrement $\\lambda(\\mathbf{x})$.\n",
    ">>    2. *Stopping criterion*. **break** if $\\lambda^2/2\\leq \\epsilon$.\n",
    ">>    3. *Choose step size $t$*: either by line search or fixed $t$.\n",
    ">>    4. *Update*. $\\mathbf{x}:=\\mathbf{x}+t \\Delta \\mathbf{x}_{nt}$.\n",
    ">\n",
    ">**output** $\\mathbf{x}$\n",
    "\n",
    "Again, the convergence can be monitored using the Newton decrement:\n",
    "\n",
    "$$\n",
    "\\lambda^2(\\mathbf{x}) = - \\Delta \\mathbf{x}_{nt}^\\top \\nabla f(\\mathbf{x})\\,.\n",
    "$$\n",
    "\n",
    "The algorithm terminates when\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda(\\mathbf{x})^2}{2} < \\epsilon\\,.\n",
    "$$\n",
    "\n",
    "The Newton decrement also indicates whether we are in or close to the feasible region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 2**\n",
    "\n",
    "1. Complete the code for the linearly constrained Newton method.\n",
    "2. Use this code to find the minimum of the non-quadratic toy problem, defined above (compare a feasible and infeasible start).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from constrained import linear_constrained_newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_constrained_newton(f, x0, grad_f,\n",
    "              hess_f, A, b, stepsize=0.25, epsilon=1e-3,\n",
    "              trace=False):\n",
    "    '''\n",
    "    Newton's method for minimizing functions with linear constraints.\n",
    "\n",
    "    Inputs:\n",
    "        - f: function to be minimized\n",
    "        - x0: starting point (does not have to be feasible)\n",
    "        - grad_f: gradient of the function to be minimized\n",
    "        - hess_f: hessian matrix of the function to be minimized\n",
    "        - A, b: linear constraints\n",
    "        - stepsize: step size for each Newton step (fixed)\n",
    "        - epsilon: parameter to determine if the algorithm is converged\n",
    "        - trace: (bool) store the path that is followed?\n",
    "\n",
    "    Outputs:\n",
    "        - xstar: the found minimum\n",
    "        - x_steps: path in the domain that is followed (if trace=True)\n",
    "        - f_steps: image of x_steps (if trace=True)\n",
    "    '''\n",
    "    assert stepsize < 1 and stepsize > 0\n",
    "    x = x0  # initial value\n",
    "    p, n = A.shape\n",
    "    if trace: x_steps = [x.copy()]\n",
    "    if trace: f_steps = [f(x0)]\n",
    "    while True:\n",
    "        ddfx = hess_f(x)\n",
    "        dfx = grad_f(x)\n",
    "        # calculate residual\n",
    "        Dx, _ = solve_constrained_quadratic_problem(... # complete!\n",
    "        newton_decrement = ...\n",
    "        if ...  # stopping criterion\n",
    "            break  # converged\n",
    "        # perform step\n",
    "        if trace: x_steps.append(x.copy())\n",
    "        if trace: f_steps.append(f(x))\n",
    "    if trace: return x, x_steps, f_steps    \n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to complete\n",
    "\n",
    "A = ... #  1 x 2 array\n",
    "b = ... #  1 x 1 array\n",
    "x0 = np.array([[0.6], [-0.2]])  # feasible\n",
    "#x0 = np.array([[0.6], [0.2]])  # unfeasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstar, x_steps, f_steps = linear_constrained_newton(..., trace=True)  # complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimizer:')\n",
    "print(xstar)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_contour(nonquadratic, (-2, 2), (-1, 1), ax, [1, 3], 0)\n",
    "add_path(ax=ax, x_steps=x_steps)\n",
    "ax.scatter(xstar[0,0], xstar[1,0], 50, 'r', label='minimum')\n",
    "ax.grid()\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Boyd, S. and Vandenberghe, L., '*[Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)*'. Cambridge University Press (2004)\n",
    "- Bishop, C., *Pattern Recognition and Machine Learning*. Springer (2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
